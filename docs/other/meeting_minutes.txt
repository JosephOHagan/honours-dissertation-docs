29/09/17: Supervisor "Meeting" / Chat
Approached Dr. Williamson during a 3rd year lab as due to the hectic academic style life she missed my email to organise a meeting. Originally I was  going to send an email during the weekend but learned she happened to be running a lab at the same time I too was in the Boyd Orr. Took a chance in going direct to the source. It seemed to work.

Dr. Williamson gave a brief overview of the aims of the project. I'll be working with Debbie and Dennis (http://dennisanddebbie.club/) who'll serve as the client role. Debbie is the more technical of the two and Dennis the artist. Dr. Williamson said we'll need to mine them for requirements (as is the software engineering way)

I'll need to think about what I'll have to ask them. Their current work flow and development tools, how they imagine feature X and Y, etc. Dr. Williamson can probably help with that or at the very least ensure I don't forget to ask something completely obvious.

Project though is to develop toolkit which requires little to none technical experience. 

Current VR toolkits are apparently poorly documented and a technical nightmare for artists (Dr. Williamson had to reverse engineer one of them in order get something working)

She said I can probably take a look at some code / work which she has done previously to get a general idea of what I'm up against.

My project is to change that and make a toolkit that is easily usable by non-technical developers. 

Spoke briefly about some things I can do in preparation of our next meeting. She recommended three existing libraries (VRTK, Newton VR, SteamVR) to take a look at as well in addition to taking a general look at Unity and C#. I've a little experience with C# and Unity but will need a refresher on that. Wait I should make Word Scrumble in Unity as a refresher!

Dr. Williamson went over the key things to do before our next meeting - setup GitHub repo, meeting minutes and other documentation, setup dissertation template and suitable link dump for the dissertation. Do some general ready of existing tools and start playing with Unity & C#. 

She also mentioned that Unity projects and GitHub repos can be troublesome to work with at times. It won't be too bad as I'm the lone developer but I should play around with that beforehand.

They have Vives and Gear VRs though the project description stated aimed at mobile devices.

Anyway John Carmack is currently working on Gear VR and if John Carmack deems something important enough to be actively working on it then that's justification enough.

THEY ARE SETTING UP A VR LAB WITHIN THE SCHOOL.

That is rad.

Scheduled our first "proper" meeting for Thursday October 5th 2.30pm

######################################################################

05/10/17: Proper Supervisor Meeting

Note: We setup a regular meeting for Wednesday 2.30pm

Went over what I've done thus far in preparation. We also went over the key goals of the project once more in some more detail.

The VR lab has yet to be setup, academics are busy people, but they hope to have it setup within the next week. Book out time slots, etc. It's going to have a Vive!

Using Vive might have an effect on making some games in VR and other side effects - use the SteamVR camera. 
It will be worth looking into SteamVR more as well and reading up on that. 
I wonder if there was a Steam Dev Days talk on it? 
Surely...

I'll need to find out what exactly I need the VR lab for (aside from the obvious). 
What work can be done at home (think Christmas break, lab is booked, etc.) as I imagine quite a bit of it can be.

##
GENERAL IDEA / PROBLEMS WITH EXISTING TOOLS
As has been covered the general idea of the project is to make VR development as non-technical developer friendly as possible. 
No coding required if possible. 
Drag and drop this script here and you're set.

Despite tools like VRTK claiming to be user friendly and easy to use they still use a lot of Unity terminology, require lots of configuration to get up and running and often a lot of digging into code to get things running. 
That isn't not artist friendly. 

Newton VR was apparently HORRIBLE especially to work with for Dr. Williamson and D&D. 
The code base is supposedly a mess. 
They had multiple versions of the project working concurrently where a feature was implemented in on and tested to make sure everything was still working and if it was then transfer it over to the main project.
Even then you still would randomly fall through parts of the floor because some other object was configure with this object and that had this effect on that object other there. Oh look the floor is rising.
Not non-technical friendly.

Let's simplify this process to require as little technical input from the artists and developers as possible. 
Ideally no coding would be required on their part. 
Code free.
Instead of configuring multiple things how about we configure or set it up once and all that other work is taken care of on the back end
It all just works and is configured for you behind the scenes
How about we just drag and drop a pre-made script and that setups up the feature that we want
Dragging and dropping scripts
Developement via a user interface and a few clicks
Think about the time required and number of steps to add an object and make it interactable - let's make that as few steps and little time as possible
Stuff like that


ESTABLISHING DEFAULTS
Now the technical purists might scream at this but think of it as automatically setting up an object with a pre-established default (which I will determined). 
If they want more control then dig into the code and change the gravity variable value or whatever it is they want to change. 
But let's follow that idea of making it as code free as possible to setup the scene or desired feature with this pre-established default set of values and settings.
"It all works and here is what it does"

Example: Think about enabling the player's jump in a video game. 
Now in Halo the gravity set for the players jump is different and more floaty / moon-like than the gravity on the other objects in the game. 
Now the default "enable gravity" setting for an object would be your standard item in the game world which if enabled for the player would give them that gravity. 
In Halo this default gravity value would be tweaked by the developer within the code to give the floaty gravity the character has within the Halo games.
For most VR experiences a single gravity setting is probably fine but should creator wish to alter the player's gravity then they can by changing the code.
That's a bit of stretch as an example but it's an example I used to describe it to someone and I'm going to reuse it now.

So we're hiding all that config from the user and just giving them a default setup of "Here's your item setup to work with in this VR environment". 
It's less customisable but its solid and it works everytime. 
There's none of the one-off issues you might face where something just doesn't work because of the order you configure the items or because some other item is configure in a certain way, etc.

I do all the heavy lifting on the backend and provide a nice, clean, well designed UI to work with on the front end. 


SOUND LIBRARIES
Another key thing to look into is the sound / audio library. 
VRTK might not have this and the one in Newton didn't work at all for Dr. Williamson's project.
Either way there are quirks with the existing systems and mine will work properly. 
That can also be a key part of the project.

Example: When a ball makes contact with a surface it makes a noise. 
Why can't this be a drag and drop of an audio file and set under what conditions it plays?

I'll need to look into that some more but it sounds feasible.
At least the simple case of attaching a sound file for the ball to make noise when it collides.
Can't be that hard.
And even a simpy system which works is better than the competition's which is complex and broken.
That complexity just makes it harder to fix and setup.
My system is user friendly


END PRODUCT
Dr. Williamson also confirmed what I initially thought about the project and that is if good enough I get to release it on the Unity asset store, as an open source project, etc.
The other tools are... well I won't use the word she used but are too clunky and technical for your average artist or non-technical developer.
VR is in its infancy again (try to forget its first infancy in the 90s)
The tech is finally at a level where it's not a blocky, Tron-like world
We can make that but think about the difference between the original Tron and Tron Legacy
So I'm shipping the base, core model of a Virtual Reality Developement Toolkit for Unity in 6-7 months
Life called my bluff

##
The general plan for the next two weeks though is as follows:

2 weeks time have a requirements gathering session thingy with Debbie & Dennis:
	- Record the interview / chat and turn that into a plan for development
	- Get them to go through a code walkthrough so they can point and go "HATED THAT PART", etc.
	- Questions like "Can you give an example of a task you would want simplified", "What's the current process", etc.
	- Ask D&D what they want streamlined (it'll probably be dev-heavy but that's great because it looks and is more impressive!)
	- Record interview
	- Turn the recorded interview into a development plan - milestones (what do I want to achieve by Christmas / goal planning in that sense)

	
HELLO WORLD IN VR (PRACTICAL VR DEVELOPMENT)
I liked the idea of make a fireworks show in VR. 
But a Hello World in VR really should include something you can pick up and throw at a wall or the ground. 
Ideally it won't go through either. 
Of course there will be physics associated with the item. 
And it would be really cool if it made a noise on collision. 
Stuff like that.

DEVELOPMENT WORK & UNITY RESEARCH
Make some Unity games
Make some Unity VR games
Understand the process of developing and releasing a similar type of tool on the Unity asset store
General understanding of VR development and what Unity provides by default
Get a good grip on C#
 
## 
Questions and their Answers:

Question: Regarding an example of a task to simplify
Answer: 
This was answered pretty much before I got the chance to ask but it again highlights the non-developer nature of the project. 
For example to add one item might require configuring like 5 different things and then you still fall through portions of the floor. 
Let's simplify that to a few clicks, drag a script here and it's set. 
None of this configure A, B, C, oh and D is broken so configure P, Q, R, etc. configure once if at all. 

######################################################################

11/10/17: Proper Supervisor Meeting

There wasn’t much to discuss at today’s meeting. Just keep on becoming a game / VR developer.

I mentioned the continued attempt to get a hold of Unity and that as well as that I’ve prepared some questions but will need to continue with that.

I’ll probably throw those in a text file and write them in my notepad prior to the meeting.

Aside from that the lab is now going to be in the lab-type of room that’s in Dr. Williamson’s office. I assisted in clearing it with her. She also had musical chairs. Literally musical chairs. You sat on them and part of a song played. When everyone sat you got the entire band and the full song. Also the orb is moving. I did get to touch it though before it left. 

Our intention was to set up the Vive and dev environment but Graham didn’t send it over to her in time. We did go looking for it in his office but it appeared to still be connected and we couldn’t find the controllers.

We eventually tracked down Graham who said he’d send it over shortly. With that we pretty much ran out of time as she had to move the orb. It’s a very large and impressive orb. We did agree to meet  tomorrow again at 2.30 to learn how to use the Vive alongside the other student she is working who needs the Vive. We might play some VR games to make sure it works correctly. This is very important work. I hope I get to play Valve’s VR Lab game. Technically it was made with Unity. If not tomorrow then I’m sure I can persuade Dr. Williamson to allow me to do so around Christmas time. I should get her a card when the time comes.

######################################################################

12/10/17: Vive Setup Meeting

Dr. Williamson showed us how to setup the Vive. I then proceeded to spend the afternoon playing Vive games and getting a feel for using it.

Tilt Brush: We painted in 3D space. It's very strange to draw in 3 dimensions. You go to draw a straight horizontal line but it curves slightly because that's the way your hand and arm moves whilst making that motion. It's very strange.

Waltz of the Wizard: Another experience type of VR thingy. You get to play around in a wizards workshop. I threw Hadokens. Also this: [INSERT GIANT BOMB WIZARD VIDEO LINK] (sorry)

Rec Room: Rec Room is incredible. 
The ability to chat to other players is not this revolutionary concept but it's incredibly strange to interact and be in complete control of your avatar. 
It's that level of control that causes you to build a floating tower of chairs in the middle of a basketball court where you stand on top of the tower, catch chairs being thrown up to you by other players, continue to grow the tower by placing your caught chair on top and then teleporting on top of it before catching another chair and repeating. Turns out while you can stick your head through the ceiling the chairs cannot go through it.
Or how about entering with a team of four into a combat minigame. Running over to your downed squad mate, reviving him with a him fire and then firing and killing an enemy running up behind him while he simultaneously takes out one running up behind you. Then we fist bumped.
Rec Room is incredible.

######################################################################

18/10/17: Supervisor Meeting

TL;DR:

I scheduled a meeting with Debbie & Dennis for next week hopefully. I thought it was this week (my bad I should have followed up on that). If they can't attend then Dr. Williamson will just run me through the NewtonVR code and get an emailed list of requirements which should suffice. 

This week I gave an update on what I've been working on (Hello World VR opposed to Hello World Unity) and will need to continue to work on that and gain a further understanding in addition to figuring out the next few things I'll need for development of this project:
- Look at loading between scenes (without default Vive room)
- Look at GUI tool development
- Look at skyboxes (VR and non-VR)
- Look at event handling (VR and non-VR)
- Look at Unity camera (VR and non-VR)

There's also a few other documentation and planning type of things to produce:
- Make a rough estimate of a project development timeline
- Make a rough estimate of a development plan
- Create a project proposal / plan document (after meeting next week)


LOADING BETWEEN SCENES
One of the issues highlighted by Dr. Williamson with their application's development was loading between scenes.
One scene ends by triggering some event which transitions into a loading screen (developed by the user) which when the scene has loaded then transitions into the next scene.
Simply right?
Well not exactly. 
You shouldn't use the typical approach for scene transitions in Unity because the frame rate is at risk (kill the frame rate at a loading screen because it doesn't really matter and the resources are better spend elsewhere)
SteamVR instead has a method to load between scenes which transitions breifly to the default loading space (which happens to be the SteamVR home space or something)
Dr. Williamson couldn't get this to go straight to their own loading screen without first going here for a few seconds.
She combat this problem by just making the entire room black.
So you saw a few seconds of complete darkness before the actual loading screen kicked in.
It would be better to go straight to the loading screen.
I need to see how to change the default space used by SteamVR and make it easily changed if it isn't.
Asynchronous loading - look into that

GUI FRONT END TOOL DEVELOPMENT
The user should be able to interact with the scripts via a GUI menu (VRTK has its own menu component like the Unity Inspection menu)
I need to figure out how to create some simple example scripts which can be interacted with via this GUI
I've the idea of "established defaults" and so it would be great to be able to change the values in a menu opposed to digging through a script
I essentially need to learn how to make a Unity GUI tool and get it so it's able to change some established default value of the script its working with

CAMERA POSITIONING
Another issue I'm going to have to look at more is how the Unity camera works (in VR and non-VR applications)
One problem Dr. Williamson, Debbie and Dennis faced when developing their application was they wanted to start user with the camera facing the other way after a scene transitions
This was apparently a lot harder and more complex than it should have been
This also was an example of some of the challenges faced with the general camera controls, camera placement and camera movement
So this idea of changing a camera position / perspective both between scenes and during scene setup
I'm going to have to look into Unity cameras quite a bit

SKYBOXES
Yet another challenge faced by Dr. Williamson was changing the skybox of the scene
Some of the reasoning for this is explained below (TL;DR: Hard coded values and stub methods)
The best she could manage was to change one of the skyboxes to a nice solid red
Pity it wasn't meant to be red
I'll need to look into skyboxes some more
Bungie makes some incredible skyboxes. Also the Spyro games on the PlayStation 1 did as well. SkyHype.

EVENT HANDLING & OBJECT INTERACTION
Another thing to know about is event handling and object interaction
Working with actions which are assoicated with objects and interactions
Interacting with objects with more precision - giant cubes are very precise
The need for more complex and detailed / fine grained object interaction
The notion of interacting with an object to tigger some event (triggering events based off some user interaction)
Example - Push a button which start and stop a timer
Example - Push a button to spawn some items in the world
Example - Make it so when to specific objects collide that a certain particle display occurs
Example - Use a key to unlock a door

MISC DEVELOPMENT THINGS
Avatar movement - teleporting and walking around a space
Rendering - just rendering in general is something I'll need to know more about (fully understand the purpose of and how to use Valve's The Labratory renderer)

DEVELOPMENT PROCESS / TIMELINE
In terms of what to have done before Christmas, Dr. Williamson suggest getting a solid, easily extended framework in place and say 3-5 key features. Additional features can be easily slotted in and with this tool kit we can easily setup a new project, interact with some objects and easily transition between scenes with a custom loading screen.
There's the core set of features needed to get a project up and running. We build from that base.
In terms of shipping on the Unity asset store (see below) earlier is better as we can incorporate feedback and metrics into the report.
Launch early with key features and chip away and patch new features in rabidly. It'll need to have some unique / novel feature to get approval though. 
I'll need to look into that. 
This idea of a lightweight, easy to setup and allowing for fast development.
Perhaps its initial release is more developer focused in that in just handles some common SteamVR tasks and a few other novel things as well such as loading screens.
I'll need to think about / look into that.

TAG LINE
"It's everything SteamVR doesn't do"
Part of the problems faced with skyboxes, scene loading, etc. is that SteamVR contains some stub methods and hard coded values. 
In theory it should be possible to do what we are attempting to do its just that the method hasn't actually been implemented.
Or we can't change that colour because despite passing in the colour we want its actually hard coded so our passed in value is never used.
Apparently a lot of the problems faced are due to lack of or poor implementation.
It's my job to either fix that or make it much, much simpler (although perhaps slightly less flexible)

Question: What platforms to target?
In terms of MoSCoW: Vive is our Must have whereas Oculus is Should / Could Have depending on how development goes
I would like it to support both Oculus and Vive but time is a constraint. 
Perhaps that's my fun project once the Oculus Go is released.
Actually what am I doing at Christmas?
Vive primary focus (might play with Oculus after doing some reading)
That might be better as a separate tool though.
A machete over a swiss army knife. "That's not a knife. This is a knife!"
[GoT 5 vs. 1 (King Robert 5 vs 1 speech)]

Question: When should this thing be on the asset store?
You want to release it so that there's enough time to get some metrics and feedback.
This covers all the bases and your back in the report.
Shows that people are actually using the product and the feedback can help form the basis for future development. 
Aim for releasing earlier rather than later.
New features can always be patched in.
I need to find out what the timing is for getting approval of a Unity patch.

######################################################################

25/10/17: Supervisor Meeting - Debbie & Dennis Edition

This week's meeting consisted of a "requirements gathering" type of session with VR artists Debbie & Dennis

See DnD_October_Meeting document for a general transcript / notes from the meeting

######################################################################

01/11/17: Supervisor Meeting 

TL;DR:

I scheduled a meeting with Debbie & Dennis for next week hopefully. I thought it was this week (my bad I should have followed up on that). If they can't attend then Dr. Williamson will just run me through the NewtonVR code and get an emailed list of requirements which should suffice. 

Some things to look into:
- The Unity Library creation process
- Additional resources and requirements for publishing on Unity asset store (make a list of what needs to be done to get approval)
- The academic hoops to jump through regarding using user feedback from Unity asset store in evaluation

Some documentation work:
- Refine requirements MoSCoW and non-functional requirements
- Refine development plan up until Christmas break
- Copy of Debbie & Dennis meeting write up onto repository

Additional tasks:
- Read through all notes made so far and generate ideas for features / development
- Get a key for the VR room (though it will be moving soon -> probably the day I pick up the key)
- Play around with some wireframing / sketches of GUI and interaction style of diagrams

STATE MACHINE
I asked about the State machine that Dr. John Williamson produced as part of the Intern project
Its as pure a developers state machine as it gets
I must stress that in its current form is a developer's state machien
Essentially its a scene transition state machine in that the developer sets it up so that one scene transitions to another given some event
So you've got a but on scenes and you set up the conditions and events to transition from one to the next
This would require setting up a GUI interface where the user simply sets the conditions to transfer from one scene to another
Its an interesting idea though I fear it will not be implemented due to the time constraints put on the project
For now will be considered a "Could Do" goal in the MoSCoW planning 
If not implemented can be one of the "Well they system could be improved / extended to include..." types of things for the dissertation

UNITY LIBRARY CREATION
Dr. Williamson seems to remember that there are a few extra steps required in Unity library creation process
I’ll need to look into this and do a simple "Hello World" style of application
In addition to this I'll need to do further investigation into the procedure for launching a product on the Unity asset store

UNITY ASSET STORE & DISSERTATION ETHICS WARNING
I intend to use user feedback from the Unity Asset store in my dissertation
Regarding the Unity asset store approval I'll need to make a list of all the things I need to do to get approval on the store
E.g. Is there a special project structure required for submission, etc.
Additionally, before the Christmas break I should make arrangements and seek out all of the ethics concerns that the university might have if I intend to use user feedback taken from the Unity asset store in my dissertation
Dr. Williamson said that it'll be fine to use but there will most likely be an extra hoop to jump through
So just figure out the ethics of using that user feedback in the dissertation before Chirstmas

GENERAL DOCUMENTATION
I'm allowed to reference the meeting notes I created based on my meeting with Debbie & Dennis last week in the dissertation itself
In general I should tidy it up somewhat, check over some of the details and flesh out some of the points
Probably compile it into a PDF and upload that alongside the raw text file on the repo
I might do this for other pieces of documentation as well just so that they are clearer and easier to read
It  will also give me a chance to review all of the notes taken thus far for ideas to help flesh out the features

DOCUMENTATION REQUIREMENTS FLESH OUT
I should continue to expand and flesh out the feature list / MoSCoW planning
At some point I'll take a look at that "dissertation Hall-of-Fame" to see how similar dissertations frame the requirements section
Regarding the feature list itself, I'll flesh out a MoSCoW list and some non-functional requirements
In terms of the actual features I'll write out what I've found so far but leave myself enough room for feature discovery

NON-FUNCTIONAL REQUIREMENT: NUMBER OF CLICKS
On the subject of non-functional I mentioned that the “number of clicks” was relative to the task being performed 
One thing to do might be to work through development using the different toolkits and count the number of clicks VRTK takes over Newtown
Really study the interaction that the user has in detail
Dr. Williamson just suggested that the non-functional requirement in that case would be along the lines of “The user’s task will be accomplished in the minimal possible amount of clicks”

WIREFRAMES & GUI DESIGN
We also spoke about the importance in creating a user friendly GUI interface for the application
A UI with helpful and informative descriptions of what each section does and what changes will do 
I should think about and investigate different methods of displaying info to the user

It might help to draw up some interaction diagrams like we did in Interactive Systems last year
Map out the possible ways and steps a user will take to complete a given task 
Iterate on this design and attempt to build the system to automate and minimise this process
It might also help to do this for existing toolkits as a means of comparison between existing systems and my own system - from a design and usability perspective 

An example is to consider the hinges used for drawers and doors
Dr. Williamson’s main point here was the horrific mess it turns the GUI into when attempting to work with them
Its information overload and you have no idea half the time what you're changing
Instead I need to think about how I could redesign this process to make it more user friendly
How can I communicate to the user what each section is for and does
How can I explain how changes to one subsection echo through the setup and might break things

In general just make the GUI in general less dense and more user friendly whilst making it explain itself better to the user
Labels and notes telling you what a particular thing is for and how changing it might break something else
Explanatory text and names to help guide the user
Stuff like that

SUPER HOT!
I reminded Dr. Williamson that Superhot VR was currently on sale as part of the Steam Halloween sale
SUPER HOT!
We tried to buy Superhot VR edition but were unsuccessful though Dr. Williamson said she would head down before the sale was over and purchase it
SUPER HOT!
I'm really looking forward to getting a chance to try out the game in VR
It's very useful to see what other developers are doing after all and take ideas

######################################################################

08/11/17: Supervisor Meeting Cancelled -- Dr. Williamson was ill

I wish her a steady recovery. 

I sent her an email with a recap of what I did and was planning to do.

I had no real questions beyond was there anything particularly interesting at some potential PhD talk event the department was holding. That and do I have access to the VR lab on weekends. Both of those can probably wait though.

######################################################################

15/11/17: Supervisor Meeting

I gave a brief overview of what I'd done in the past two weeks and explained my plan through to the start of the next year and the submission for asset store approval.

I explained that I'd not had as much of a chance to work on the project than I would have liked due to other coursework, though Dr. Williamson assured me that I was going ok and not to worry - comforting somewhat.

I requested weekend access to the VR lab to assist with the development process and that should be taken care of within the next week. I will need to check the card at somepoint in the near future and that it has been correctly activated.

Additionally Dr. Williamson talked about a conference the university was holding in her subject area. She mentioned a lot of interesting things which were going on and being worked on. Someone is investigating the change in taste due to visual and audio effects. Another group are looking at speech interaction in VR through a recreation of the Boston marathon bombing where you have to talk to people to progress the story and experience. That one is a little out there. I should look into things like that more.

######################################################################

22/11/17: Supervisor Meeting

There wasn't that much to talk about this week. The term is coming to a close and so things are getting frantic for both the students and the academics. 

Good to know it wasn't just me.

I mentioned the whole London interview thing. Dr. Williamson agrees that London can be TOO MUCH. It's very dense. It's very active. To quote one fellow interviewee when I commented on the size and magnitude of the Amazon building "Your in London mate"

We spoke breifly about her conference and that type of event in general. She made a profit and got the academic equivalent of brownie points. That's good.

It all seemed pretty interesting. For the one she is attending next week the "crazy idea" paper was to attach a robotic finger to a pair of glasses or the top of a phone. That's certainly interesting. I should look into that and her field some more.

Note: Next week's meeting will be rescheduled to Thursday due to Dr. Williamson going to another conference. 

######################################################################

30/11/17: Supervisor Meeting

I gave a quick update on the work that was done. I got the new lab setup login details sorted as well. I mentioned that I was thinking a lot about how to structure the toolkit in general, Dr. Williamson wasn't quite sure of the best approach so I'll spend a day or two playing around with that before our next meeting and dev-week-1 starts.

######################################################################

06/12/17: Supervisor Meeting

I updated Dr. Williamson on all of the work I've done having lived in the lab for the best part of the week. This ranged from a progress report on the current interaction systems and explained how I want to architect the system. Essentailly rather than the system employed by VRTK and NewtonVR since I am restricting myself to use the HTC Vive I streamline the setup process. Since using the Vive gives access to the SteamVR library setup and streamlining scripts can be written for that in addition to the more general scripts which I've written to achieve the same purpose. That's very vauge. I'll explain this better elsewhere. Basically my toolkit is called SVRA (SteamVR Assistant) which aims to contain 5 sub-toolkits which provide different features for VR development.

Aside from the work update Dr. Williamson asked how I was intending on evaluating the system. She suggested that aside from the public release (which we'll probably have to get Matthew Chalmers to sign off on but should be fine) that we do an evaluation with VR developers (staff VR people / Debbie & Dennis) where we set them some simple tasks and monitor their progress. Afterwards a quick discussion comparing it to existing toolkits. In addition to that I was intending on looking into evaluation methods over the Christmas break and planning it out then.

NOTE: This will be our last meeting of 2017

UPDATE: Some guy online sent me an email saying he was looking for a simple, streamlined VR dev toolkit for teaching VR to new developers. VRTK is too dense, etc. but he was wondering why I didn't just write a VRTK wrapper. I should probably mention this to Dr. Williamson. I'll email him first as this could prove very useful.